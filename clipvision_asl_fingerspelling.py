# -*- coding: utf-8 -*-
"""clipvision_asl_fingerspelling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SHz-t2I9DKyxEbC9F7C4nKdhVSZyUXSJ
"""

!pip install kaggle datasets transformers torch torchvision

# Import necessary libraries
import os
import zipfile
import shutil
from PIL import Image
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from torch.optim import AdamW
from sklearn.metrics import accuracy_score, f1_score
from tqdm import tqdm
from transformers import AutoProcessor, CLIPVisionModel
from huggingface_hub import HfApi

# Early preprocessing of the data which includes signs other than the alphabet and a test dataset which is not usable due to its small size.

# Download the dataset from Kaggle
!kaggle datasets download -d debashishsau/aslamerican-sign-language-aplhabet-dataset

# Unzip the file
with zipfile.ZipFile('/content/aslamerican-sign-language-aplhabet-dataset.zip', 'r') as zip_ref:
    zip_ref.extractall('/content/aslamerican-sign-language-aplhabet-dataset')

# Remove images which are not relevant
dir_to_remove = ['/content/aslamerican-sign-language-aplhabet-dataset/ASL_Alphabet_Dataset/asl_alphabet_train/del',
                 '/content/aslamerican-sign-language-aplhabet-dataset/ASL_Alphabet_Dataset/asl_alphabet_train/nothing',
                 '/content/aslamerican-sign-language-aplhabet-dataset/ASL_Alphabet_Dataset/asl_alphabet_train/space'] # list of unnecessary directories

# Go through the list and remove
for path in dir_to_remove:
    if os.path.exists(path):
        shutil.rmtree(path)

# Set path to the dataset directory
dataset_dir = "/content/aslamerican-sign-language-aplhabet-dataset/ASL_Alphabet_Dataset/asl_alphabet_train"

# Retrieve image paths and corresponding labels from the subdirectory names
image_paths = []
labels = []

# Go through subdirectories
for label in os.listdir(dataset_dir):
    class_dir = os.path.join(dataset_dir, label) # create path to subdirectory
    if os.path.isdir(class_dir): # check if it's a directory, not a file
        print(f"Processing class: {label}") # see processed classes for debugging
        for image_name in os.listdir(class_dir):
            if image_name.endswith((".jpg", ".jpeg", ".png")): # go through only files that are images
                image_path = os.path.join(class_dir, image_name) # create path to file
                image_paths.append(image_path)
                labels.append(label)

# Create a mapping of class labels to numeric indices
class_names = sorted(os.listdir(dataset_dir))  # Get all class names
label2index = {label: idx for idx, label in enumerate(class_names)}  # Map labels to indices

# Prepare dataset class for preprocessing
class ASLDataset(Dataset):
    def __init__(self, image_paths, labels, processor, label2index):
        self.image_paths = image_paths
        self.labels = labels
        self.processor = processor
        self.label2index = label2index

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        # Load the image
        image = Image.open(self.image_paths[idx]).convert("RGB")

        # Convert label (text) to numeric index
        text_label = self.labels[idx]
        numeric_label = self.label2index[text_label]

        # Preprocess the image using the processor
        inputs = self.processor(images=image, return_tensors="pt", padding=True)

        # Return pixel values and label
        return {
            "pixel_values": inputs["pixel_values"].squeeze(0),
            "label": numeric_label
        }

# Initialize CLIP processor
processor = AutoProcessor.from_pretrained("openai/clip-vit-base-patch32")
dataset = ASLDataset(image_paths=image_paths, labels=labels, processor=processor, label2index=label2index)

# Split the dataset 70/20/10%
train_size = int(0.7 * len(dataset))
val_size = int(0.2 * len(dataset))
test_size = len(dataset) - train_size - val_size
train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])

# Check sizes of the split dataset
print(f"Train size: {train_size}")
print(f"Validation size: {val_size}")
print(f"Test size: {test_size}")

#Create Dataloaders for train, validation, and test sets
batch_size = 32

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Check the shape of a batch
batch = next(iter(train_loader))
print(batch)

# Initialize CLIPVision
clip_model = CLIPVisionModel.from_pretrained("openai/clip-vit-base-patch32")

# Define module with a classifier head that maps image features from CLIPVision to the number of classes
class CLIPVisionClassifier(nn.Module):
    def __init__(self, clip_model, num_classes):
        super().__init__()
        self.model = clip_model
        self.classifier = nn.Linear(self.model.config.hidden_size, num_classes)

    def forward(self, pixel_values):
        outputs = self.model(pixel_values=pixel_values)  # Forward pass to get image features
        image_features = outputs.pooler_output  # Get the pooled feature representation

        # Pass the feature vector through the classifier
        logits = self.classifier(image_features)
        return logits


# Initialise model with classifier
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = CLIPVisionClassifier(clip_model=clip_model, num_classes=26).to(device)

# Assign optimizer, scheduler and loss function
criterion = nn.CrossEntropyLoss()
optimizer = AdamW(model.parameters(), lr=1e-5)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)

# Training loop
def train(model, dataloader, criterion, optimizer, scheduler, device):
    model.train()
    running_loss = 0.0
    total_batches = len(dataloader)

    for batch_idx, batch in enumerate(dataloader):
        pixel_values = batch["pixel_values"].to(device)
        labels = batch["label"].to(device)  # Numeric labels

        optimizer.zero_grad()

        # Forward pass
        logits = model(pixel_values=pixel_values)  # returns logits from the classifier head

        # Compute loss
        loss = criterion(logits, labels)
        loss.backward()

        # Gradient clipping and optimization
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)
        optimizer.step()

        running_loss += loss.item()

    scheduler.step()
    epoch_loss = running_loss / total_batches
    print(f"Training Loss: {epoch_loss:.4f}")

    return epoch_loss

def evaluate(model, dataloader, criterion, device):
    model.eval()
    total_correct = 0
    total_samples = 0
    val_loss = 0.0

    with torch.no_grad():
        for batch in dataloader:
            pixel_values = batch["pixel_values"].to(device)
            labels = batch["label"].to(device)

            # Forward pass
            logits = model(pixel_values=pixel_values)

            # Predicted labels
            predicted_indices = torch.argmax(logits, dim=1)

            # Calculate metrics
            correct_predictions = (predicted_indices == labels).sum().item()
            total_correct += correct_predictions
            total_samples += len(labels)

            # Compute validation loss
            loss = criterion(logits, labels)
            val_loss += loss.item()

    # Compute overall metrics
    accuracy = total_correct / total_samples
    avg_val_loss = val_loss / len(dataloader)

    print(f"Validation Loss: {avg_val_loss:.4f}")
    print(f"Validation Accuracy: {accuracy:.4f}")

    return avg_val_loss, accuracy

# Set number of epochs
num_epochs = 10

# Training and evaluation
for epoch in range(num_epochs):
    print(f"Epoch {epoch+1}/{num_epochs}")
    train_loss = train(
        model=model,
        dataloader=tqdm(train_loader, desc=f"Epoch {epoch+1}"),
        criterion=criterion,
        optimizer=optimizer,
        scheduler=scheduler,
        device=device
    )

    val_loss, val_accuracy = evaluate(
    model=model,
    dataloader=val_loader,
    criterion=criterion,
    device=device
    )

# Save the entire model weights (including the classifier head)
torch.save(model.state_dict(), "clipvision-asl-fingerspelling.pth")

# Save the configuration of CLIPVision model
model.model.config.save_pretrained("clipvision-asl-fingerspelling")

# Save the processor
processor.save_pretrained("clipvision-asl-fingerspelling")

# Push to HuggingFace
api = HfApi()
token = ''

api.upload_file(
    path_or_fileobj="clipvision-asl-fingerspelling.pth",
    path_in_repo="pytorch_model.bin",
    repo_id="aalof/clipvision-asl-fingerspelling",
    token=token
)

api.upload_folder(
    folder_path="clipvision-asl-fingerspelling",
    repo_id="aalof/clipvision-asl-fingerspelling",
    token=token
)

def test(model, dataloader, device, index2label):
    model.eval()
    total_correct = 0
    total_samples = 0
    all_predictions = []
    all_true_labels = []

    # For storing class-wise F1 scores
    per_class_f1 = {}

    with torch.no_grad():
        for batch in dataloader:
            pixel_values = batch["pixel_values"].to(device)
            labels = batch["label"].to(device)

            # Forward pass
            logits = model(pixel_values=pixel_values)

            # Predicted labels
            predicted_indices = torch.argmax(logits, dim=1)

            # Collect predictions and true labels for F1 score calculation
            all_predictions.extend(predicted_indices.cpu().tolist())
            all_true_labels.extend(labels.cpu().tolist())

            # Calculate accuracy
            correct_predictions = (predicted_indices == labels).sum().item()
            total_correct += correct_predictions
            total_samples += len(labels)

    # Compute overall accuracy
    accuracy = total_correct / total_samples

    # Compute weighted F1 score
    weighted_f1 = f1_score(all_true_labels, all_predictions, average='weighted', zero_division=0)

    # Compute per-class F1 scores (not weighted)
    class_f1_scores = f1_score(all_true_labels, all_predictions, average=None, labels=list(index2label.keys()), zero_division=0)

    # Map F1 scores to actual class labels
    for i, score in zip(index2label.keys(), class_f1_scores):
        per_class_f1[index2label[i]] = score

    # Print the results
    print(f"Test Accuracy: {accuracy:.4f}")
    print(f"Test Weighted F1 Score: {weighted_f1:.4f}")

    # Print F1 scores per class (using actual class names)
    for class_name, f1 in per_class_f1.items():
        print(f"{class_name} - F1 Score: {f1:.4f}")

    return accuracy, weighted_f1, per_class_f1

# Reversing the label2index to get index2label
index2label = {idx: label for label, idx in label2index.items()}

# Testing
test_accuracy, test_weighted_f1, test_per_class_f1 = test(
    model=model,
    dataloader=test_loader,
    device=device,
    index2label=index2label
)