{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_hfF_VIFCQb",
        "outputId": "90f9459c-92e8-422c-c4ec-db2efc339bd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.6.17)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.3.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle) (3.10)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.11/dist-packages (from python-slugify->kaggle) (1.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle datasets transformers torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoProcessor, CLIPVisionModel"
      ],
      "metadata": {
        "id": "pMsBVMDzkrxo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Early preprocessing of the data which includes signs other than the alphabet and a test dataset which is not usable due to its small size.\n",
        "\n",
        "# Download the dataset from Kaggle\n",
        "!kaggle datasets download -d debashishsau/aslamerican-sign-language-aplhabet-dataset\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile('/content/aslamerican-sign-language-aplhabet-dataset.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/aslamerican-sign-language-aplhabet-dataset')\n",
        "\n",
        "# Remove images which are not relevant\n",
        "dir_to_remove = ['/content/aslamerican-sign-language-aplhabet-dataset/ASL_Alphabet_Dataset/asl_alphabet_train/del',\n",
        "                 '/content/aslamerican-sign-language-aplhabet-dataset/ASL_Alphabet_Dataset/asl_alphabet_train/nothing',\n",
        "                 '/content/aslamerican-sign-language-aplhabet-dataset/ASL_Alphabet_Dataset/asl_alphabet_train/space'] # list of unnecessary directories\n",
        "\n",
        "# Go through the list and remove\n",
        "for path in dir_to_remove:\n",
        "    if os.path.exists(path):\n",
        "        shutil.rmtree(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGHccSehP371",
        "outputId": "2f1736ac-e14b-4695-eb01-ac1c9b393747"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/debashishsau/aslamerican-sign-language-aplhabet-dataset\n",
            "License(s): CC0-1.0\n",
            "Downloading aslamerican-sign-language-aplhabet-dataset.zip to /content\n",
            "100% 4.19G/4.20G [00:55<00:00, 165MB/s]\n",
            "100% 4.20G/4.20G [00:55<00:00, 80.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set path to the dataset directory\n",
        "dataset_dir = \"/content/aslamerican-sign-language-aplhabet-dataset/ASL_Alphabet_Dataset/asl_alphabet_train\"\n",
        "\n",
        "# Retrieve image paths and corresponding labels from the subdirectory names\n",
        "image_paths = []\n",
        "labels = []\n",
        "\n",
        "# Go through subdirectories\n",
        "for label in os.listdir(dataset_dir):\n",
        "    class_dir = os.path.join(dataset_dir, label) # create path to subdirectory\n",
        "    if os.path.isdir(class_dir): # check if it's a directory, not a file\n",
        "        print(f\"Processing class: {label}\") # see processed classes for debugging\n",
        "        for image_name in os.listdir(class_dir):\n",
        "            if image_name.endswith((\".jpg\", \".jpeg\", \".png\")): # go through only files that are images\n",
        "                image_path = os.path.join(class_dir, image_name) # create path to file\n",
        "                image_paths.append(image_path)\n",
        "                labels.append(label)\n",
        "\n",
        "# Create a mapping of class labels to numeric indices\n",
        "class_names = sorted(os.listdir(dataset_dir))  # Get all class names\n",
        "label2index = {label: idx for idx, label in enumerate(class_names)}  # Map labels to indices"
      ],
      "metadata": {
        "id": "kBWO1vBVttUr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92f51399-923d-404f-dee8-0028b5056f80"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing class: V\n",
            "Processing class: U\n",
            "Processing class: E\n",
            "Processing class: R\n",
            "Processing class: A\n",
            "Processing class: D\n",
            "Processing class: F\n",
            "Processing class: J\n",
            "Processing class: G\n",
            "Processing class: C\n",
            "Processing class: B\n",
            "Processing class: P\n",
            "Processing class: X\n",
            "Processing class: H\n",
            "Processing class: L\n",
            "Processing class: Z\n",
            "Processing class: Q\n",
            "Processing class: M\n",
            "Processing class: W\n",
            "Processing class: S\n",
            "Processing class: K\n",
            "Processing class: T\n",
            "Processing class: O\n",
            "Processing class: N\n",
            "Processing class: Y\n",
            "Processing class: I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dataset class for preprocessing\n",
        "class ASLDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, processor, label2index):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.processor = processor\n",
        "        self.label2index = label2index\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load the image\n",
        "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "\n",
        "        # Convert label (text) to numeric index\n",
        "        text_label = self.labels[idx]\n",
        "        numeric_label = self.label2index[text_label]\n",
        "\n",
        "        # Preprocess the image using the processor\n",
        "        inputs = self.processor(images=image, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "        # Return pixel values and label\n",
        "        return {\n",
        "            \"pixel_values\": inputs[\"pixel_values\"].squeeze(0),\n",
        "            \"label\": numeric_label\n",
        "        }\n",
        "\n",
        "# Initialize CLIP processor\n",
        "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "dataset = ASLDataset(image_paths=image_paths, labels=labels, processor=processor, label2index=label2index)\n",
        "\n",
        "# # Split the dataset 70/20/10%\n",
        "# train_size = int(0.7 * len(dataset))\n",
        "# val_size = int(0.2 * len(dataset))\n",
        "# test_size = len(dataset) - train_size - val_size\n",
        "# train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# # Check sizes of the split dataset\n",
        "# print(f\"Train size: {train_size}\")\n",
        "# print(f\"Validation size: {val_size}\")\n",
        "# print(f\"Test size: {test_size}\")\n",
        "\n",
        "# Create Dataloaders for train, validation, and test sets\n",
        "#batch_size = 32\n",
        "\n",
        "#train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "#val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "#test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "r3H2wC7jYcCn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and split a small subset to test\n",
        "subset, _ = random_split(dataset, [10000, len(dataset) - 10000]) # random_split to get instances of several letters\n",
        "sub_val_size = int(0.2 * len(subset))\n",
        "sub_train_size = len(subset) - sub_val_size\n",
        "sub_train_dataset, sub_val_dataset = random_split(subset, [sub_train_size, sub_val_size]) #split into 80/20 train and validation sets\n",
        "\n",
        "# Set batch size and create Dataloaders for the subset\n",
        "batch_size = 32\n",
        "\n",
        "sub_train_loader = DataLoader(sub_train_dataset, batch_size=batch_size, shuffle=True)\n",
        "sub_val_loader = DataLoader(sub_val_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "9flGVoT2bSNw"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shape of a batch\n",
        "batch = next(iter(sub_train_loader))\n",
        "print(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B916nhe6bWnA",
        "outputId": "0744f91a-c087-4ea8-e4ad-481f47384d00"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'pixel_values': tensor([[[[-1.7923, -1.7923, -1.7485,  ..., -1.6609, -1.7923, -1.7631],\n",
            "          [-1.7923, -1.7777, -1.7631,  ..., -1.6463, -1.6609, -1.7631],\n",
            "          [-1.7485, -1.7631, -0.8872,  ..., -0.6244, -0.8580, -1.5733],\n",
            "          ...,\n",
            "          [-1.7631, -1.7047, -1.5295,  ...,  0.3391, -0.0113, -1.6025],\n",
            "          [-1.7923, -1.7631, -1.6171,  ..., -0.0550, -0.4054, -1.6609],\n",
            "          [-1.7923, -1.7923, -1.6171,  ..., -1.5441, -1.5879, -1.6755]],\n",
            "\n",
            "         [[-1.7071, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.6921],\n",
            "          [-1.7221, -1.6921, -1.7521,  ..., -1.7521, -1.7071, -1.7521],\n",
            "          [-1.7521, -1.7371, -0.8216,  ..., -0.8516, -0.9867, -1.6771],\n",
            "          ...,\n",
            "          [-1.7371, -1.6771, -1.4970,  ...,  0.2589, -0.0562, -1.6470],\n",
            "          [-1.7371, -1.7071, -1.6020,  ..., -0.0862, -0.3864, -1.6470],\n",
            "          [-1.7221, -1.7221, -1.6020,  ..., -1.6020, -1.6170, -1.6320]],\n",
            "\n",
            "         [[ 2.1459,  2.0748,  2.0464,  ...,  2.0606,  2.0179,  2.0464],\n",
            "          [ 2.1032,  1.9753,  1.6340,  ...,  1.4349,  1.6624,  1.6766],\n",
            "          [ 2.0037,  1.6055,  1.5771,  ...,  0.9941,  1.4633,  1.0794],\n",
            "          ...,\n",
            "          [ 2.1032,  1.5629,  0.4537,  ...,  1.2500,  1.7620,  0.6101],\n",
            "          [ 2.1032,  1.7335,  0.9799,  ...,  1.7193,  1.9184,  0.9656],\n",
            "          [ 2.1032,  1.8046,  1.2500,  ...,  0.6670,  1.0367,  1.1363]]],\n",
            "\n",
            "\n",
            "        [[[-1.7923, -1.7631, -1.7193,  ..., -1.7193, -1.7485, -1.7485],\n",
            "          [-1.7923, -1.7193, -1.7047,  ..., -1.6317, -1.6609, -1.7777],\n",
            "          [-1.7777, -1.7193, -1.7193,  ..., -0.9893, -1.2375, -1.6317],\n",
            "          ...,\n",
            "          [-1.7193, -1.7777, -0.2302,  ..., -0.9456, -1.1207, -1.4127],\n",
            "          [-1.7923, -1.7777, -0.5368,  ..., -1.1791, -1.3251, -1.5149],\n",
            "          [-1.7777, -1.7193, -1.6317,  ..., -1.4127, -1.5003, -1.6025]],\n",
            "\n",
            "         [[-1.7521, -1.6921, -1.7371,  ..., -1.7521, -1.7521, -1.7221],\n",
            "          [-1.7071, -1.6621, -1.7371,  ..., -1.7221, -1.7071, -1.7521],\n",
            "          [-1.7371, -1.6921, -1.7371,  ..., -1.0767, -1.2869, -1.6771],\n",
            "          ...,\n",
            "          [-1.7221, -1.7521, -0.1463,  ..., -1.2718, -1.3619, -1.6470],\n",
            "          [-1.7521, -1.7521, -0.4914,  ..., -1.4519, -1.4820, -1.6320],\n",
            "          [-1.7371, -1.6921, -1.6470,  ..., -1.6470, -1.6020, -1.6771]],\n",
            "\n",
            "         [[ 2.0606,  2.1317,  2.0748,  ...,  1.9610,  2.0748,  2.1317],\n",
            "          [ 2.0748,  2.0179,  1.6766,  ...,  1.4776,  1.7477,  1.7762],\n",
            "          [ 1.9895,  1.6340,  0.7239,  ...,  0.8377,  1.2643,  1.1647],\n",
            "          ...,\n",
            "          [ 2.0179,  1.3496,  1.5487,  ..., -0.3284,  0.4395,  0.5959],\n",
            "          [ 1.9753,  1.6198,  1.8899,  ...,  0.3542,  0.8519,  0.9230],\n",
            "          [ 2.0464,  1.7477,  1.1078,  ...,  0.5248,  0.9230,  1.0225]]],\n",
            "\n",
            "\n",
            "        [[[-1.7923, -1.7777, -1.7631,  ..., -1.6901, -1.7923, -1.7923],\n",
            "          [-1.7193, -1.7923, -1.7485,  ..., -1.7485, -1.7631, -1.7047],\n",
            "          [-1.6901, -1.7047, -0.7704,  ..., -0.4784, -0.7266, -1.6609],\n",
            "          ...,\n",
            "          [-1.7339, -1.7923,  0.5581,  ...,  0.7917,  0.4267, -1.6609],\n",
            "          [-1.7777, -1.7923,  0.2077,  ...,  0.3975, -0.0259, -1.7485],\n",
            "          [-1.7777, -1.7923, -1.7923,  ..., -1.6317, -1.7193, -1.7193]],\n",
            "\n",
            "         [[-1.7521, -1.7221, -1.7371,  ..., -1.7371, -1.7521, -1.7221],\n",
            "          [-1.6771, -1.7521, -1.7371,  ..., -1.7521, -1.7371, -1.6621],\n",
            "          [-1.7371, -1.7521, -0.8066,  ..., -0.5215, -0.7166, -1.6771],\n",
            "          ...,\n",
            "          [-1.7521, -1.7521,  0.8292,  ...,  0.7692,  0.4691, -1.6320],\n",
            "          [-1.7521, -1.7521,  0.4240,  ...,  0.4090,  0.0638, -1.6621],\n",
            "          [-1.7371, -1.7071, -1.6470,  ..., -1.6621, -1.6320, -1.5870]],\n",
            "\n",
            "         [[ 2.1032,  2.1032,  2.0464,  ...,  2.0321,  2.0606,  2.1032],\n",
            "          [ 2.1459,  1.9184,  1.5771,  ...,  1.3780,  1.6482,  1.8331],\n",
            "          [ 2.0606,  1.5913,  1.4776,  ...,  1.2643,  1.7193,  1.1363],\n",
            "          ...,\n",
            "          [ 2.0321,  1.4207,  1.9895,  ...,  1.7193,  2.0464,  0.6812],\n",
            "          [ 2.1175,  1.6624,  1.9042,  ...,  2.0037,  1.8899,  0.9941],\n",
            "          [ 2.1459,  1.8188,  1.1363,  ...,  0.6670,  1.0083,  1.1789]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 1.6238,  1.6238,  1.6238,  ...,  1.6238,  1.6238,  1.6238],\n",
            "          [ 1.6238,  1.6238,  1.6238,  ...,  1.6238,  1.6238,  1.6238],\n",
            "          [ 1.6238,  1.6238,  1.6238,  ...,  1.6238,  1.6238,  1.6238],\n",
            "          ...,\n",
            "          [ 1.6238,  1.6238,  1.6238,  ...,  1.6238,  1.6238,  1.6238],\n",
            "          [ 1.6238,  1.6238,  1.6238,  ...,  1.6238,  1.6238,  1.6238],\n",
            "          [ 1.6238,  1.6238,  1.6238,  ...,  1.6238,  1.6238,  1.6238]],\n",
            "\n",
            "         [[ 1.7447,  1.7447,  1.7447,  ...,  1.7447,  1.7447,  1.7447],\n",
            "          [ 1.7447,  1.7447,  1.7447,  ...,  1.7447,  1.7447,  1.7447],\n",
            "          [ 1.7447,  1.7447,  1.7447,  ...,  1.7447,  1.7447,  1.7447],\n",
            "          ...,\n",
            "          [ 1.7447,  1.7447,  1.7447,  ...,  1.7447,  1.7447,  1.7447],\n",
            "          [ 1.7447,  1.7447,  1.7447,  ...,  1.7447,  1.7447,  1.7447],\n",
            "          [ 1.7447,  1.7447,  1.7447,  ...,  1.7447,  1.7447,  1.7447]],\n",
            "\n",
            "         [[ 1.8046,  1.8046,  1.8046,  ...,  1.8046,  1.8046,  1.8046],\n",
            "          [ 1.8046,  1.8046,  1.8046,  ...,  1.8046,  1.8046,  1.8046],\n",
            "          [ 1.8046,  1.8046,  1.8046,  ...,  1.8046,  1.8046,  1.8046],\n",
            "          ...,\n",
            "          [ 1.8046,  1.8046,  1.8046,  ...,  1.8046,  1.8046,  1.8046],\n",
            "          [ 1.8046,  1.8046,  1.8046,  ...,  1.8046,  1.8046,  1.8046],\n",
            "          [ 1.8046,  1.8046,  1.8046,  ...,  1.8046,  1.8046,  1.8046]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3099,  0.3537,  0.4121,  ...,  0.1785,  0.1785,  0.1931],\n",
            "          [ 0.3099,  0.3537,  0.4267,  ...,  0.2369,  0.2223,  0.2223],\n",
            "          [ 0.2953,  0.3245,  0.3975,  ...,  0.2369,  0.2515,  0.2223],\n",
            "          ...,\n",
            "          [-1.2083, -1.2083, -1.1937,  ..., -0.6390, -0.6244, -0.5952],\n",
            "          [-1.2375, -1.2229, -1.2083,  ..., -0.5806, -0.5514, -0.5222],\n",
            "          [-1.2229, -1.2229, -1.1937,  ..., -0.5368, -0.4784, -0.4492]],\n",
            "\n",
            "         [[ 0.6792,  0.7242,  0.7992,  ..., -0.1012, -0.1313, -0.1463],\n",
            "          [ 0.6942,  0.7392,  0.8142,  ..., -0.0712, -0.1012, -0.1463],\n",
            "          [ 0.7092,  0.7542,  0.8442,  ..., -0.1163, -0.1313, -0.1463],\n",
            "          ...,\n",
            "          [-0.9867, -0.9867, -1.0017,  ..., -0.8816, -0.8967, -0.8666],\n",
            "          [-1.0167, -1.0017, -1.0167,  ..., -0.8216, -0.8366, -0.8066],\n",
            "          [-1.0017, -1.0017, -1.0017,  ..., -0.7916, -0.7766, -0.7616]],\n",
            "\n",
            "         [[ 1.0225,  1.0652,  1.1363,  ..., -0.5417, -0.5844, -0.5844],\n",
            "          [ 1.0367,  1.0794,  1.1505,  ..., -0.5275, -0.5559, -0.6128],\n",
            "          [ 1.0225,  1.0510,  1.1221,  ..., -0.5844, -0.5986, -0.6270],\n",
            "          ...,\n",
            "          [-0.7408, -0.7266, -0.6981,  ..., -1.0110, -0.9967, -0.9683],\n",
            "          [-0.7550, -0.7408, -0.7123,  ..., -0.9825, -0.9541, -0.9256],\n",
            "          [-0.7408, -0.7408, -0.7123,  ..., -0.9399, -0.8972, -0.8688]]],\n",
            "\n",
            "\n",
            "        [[[-1.7923, -1.7485, -1.7485,  ..., -1.7047, -1.7777, -1.7777],\n",
            "          [-1.7339, -1.7923, -1.7485,  ..., -1.7485, -1.7923, -1.7485],\n",
            "          [-1.7339, -1.6901, -0.9018,  ..., -0.0696, -0.2886, -1.6755],\n",
            "          ...,\n",
            "          [-1.7777, -1.7631,  0.2369,  ..., -0.1718, -0.4784, -1.4711],\n",
            "          [-1.7631, -1.7923, -0.1426,  ..., -0.4784, -0.7558, -1.4565],\n",
            "          [-1.7923, -1.7777, -1.7193,  ..., -1.4273, -1.4857, -1.6025]],\n",
            "\n",
            "         [[-1.7521, -1.7221, -1.7521,  ..., -1.7521, -1.7521, -1.7371],\n",
            "          [-1.7221, -1.7521, -1.7371,  ..., -1.7521, -1.7521, -1.7071],\n",
            "          [-1.7521, -1.7221, -0.9117,  ..., -0.0262, -0.2363, -1.6621],\n",
            "          ...,\n",
            "          [-1.7521, -1.7521,  0.3490,  ..., -0.5065, -0.7466, -1.7221],\n",
            "          [-1.7221, -1.7521, -0.0412,  ..., -0.7466, -0.9417, -1.6020],\n",
            "          [-1.7371, -1.7071, -1.6621,  ..., -1.6771, -1.6470, -1.7221]],\n",
            "\n",
            "         [[ 2.1317,  2.1317,  2.0037,  ...,  2.0037,  2.0464,  2.0748],\n",
            "          [ 2.1459,  1.9184,  1.5629,  ...,  1.3780,  1.6055,  1.7477],\n",
            "          [ 2.0890,  1.6909,  1.4065,  ...,  1.6482,  1.9468,  1.1078],\n",
            "          ...,\n",
            "          [ 2.0179,  1.4918,  2.0464,  ...,  0.4110,  1.0510,  0.5106],\n",
            "          [ 2.0890,  1.6624,  1.9184,  ...,  1.0510,  1.4065,  0.9941],\n",
            "          [ 2.0748,  1.8046,  1.2216,  ...,  0.5675,  0.9941,  1.0794]]]]), 'label': tensor([13, 15,  1,  2,  2, 16,  3, 21,  6, 13, 21,  9, 22,  2, 20,  6, 20, 24,\n",
            "        11, 17, 25,  7, 17, 12, 25, 20, 17, 23, 25,  5, 17, 22])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize CLIPVision\n",
        "clip_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Define module with a classifier head that maps image features from CLIPVision to the number of classes\n",
        "class CLIPVisionClassifier(nn.Module):\n",
        "    def __init__(self, clip_model, num_classes):\n",
        "        super().__init__()\n",
        "        self.model = clip_model\n",
        "        self.classifier = nn.Linear(self.model.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        outputs = self.model(pixel_values=pixel_values)  # Forward pass to get image features\n",
        "        image_features = outputs.pooler_output  # Get the pooled feature representation (aggregated image features)\n",
        "\n",
        "        # Pass the feature vector through the classifier\n",
        "        logits = self.classifier(image_features)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# Initialize model with classifier\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CLIPVisionClassifier(clip_model=clip_model, num_classes=26).to(device)\n",
        "\n",
        "# Assign optimizer, scheduler and loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
      ],
      "metadata": {
        "id": "RBjrrctBbbsA"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "def train(model, dataloader, criterion, optimizer, scheduler, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    total_batches = len(dataloader)\n",
        "\n",
        "    for batch_idx, batch in enumerate(dataloader):\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)  # Numeric labels\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(pixel_values=pixel_values)  # returns logits from the classifier head\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping and optimization\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "    epoch_loss = running_loss / total_batches\n",
        "    print(f\"Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    return epoch_loss\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            pixel_values = batch[\"pixel_values\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(pixel_values=pixel_values)\n",
        "\n",
        "            # Predicted labels\n",
        "            predicted_indices = torch.argmax(logits, dim=1)\n",
        "\n",
        "            # Calculate metrics\n",
        "            correct_predictions = (predicted_indices == labels).sum().item()\n",
        "            total_correct += correct_predictions\n",
        "            total_samples += len(labels)\n",
        "\n",
        "            # Compute validation loss\n",
        "            loss = criterion(logits, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    # Compute overall metrics\n",
        "    accuracy = total_correct / total_samples\n",
        "    avg_val_loss = val_loss / len(dataloader)\n",
        "\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    return avg_val_loss, accuracy"
      ],
      "metadata": {
        "id": "xF3cJp_mcZvk"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "num_epochs = 10\n",
        "\n",
        "# Training and evaluation\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    train_loss = train(\n",
        "        model=model,\n",
        "        dataloader=tqdm(sub_train_loader, desc=f\"Epoch {epoch+1}\"),\n",
        "        criterion=criterion,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    val_loss, val_accuracy = evaluate(\n",
        "    model=model,\n",
        "    dataloader=sub_val_loader,\n",
        "    criterion=criterion,\n",
        "    device=device\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8d99418-bd34-4359-b98e-3215ec7ac323",
        "id": "BB9aYuZmYrkC"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 250/250 [01:44<00:00,  2.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.6637\n",
            "Validation Loss: 0.1736\n",
            "Validation Accuracy: 0.9510\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 250/250 [01:45<00:00,  2.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0986\n",
            "Validation Loss: 0.1285\n",
            "Validation Accuracy: 0.9625\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 250/250 [01:44<00:00,  2.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0443\n",
            "Validation Loss: 0.1410\n",
            "Validation Accuracy: 0.9635\n",
            "Epoch 4/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 250/250 [01:44<00:00,  2.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0288\n",
            "Validation Loss: 0.1129\n",
            "Validation Accuracy: 0.9690\n",
            "Epoch 5/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 250/250 [01:44<00:00,  2.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0164\n",
            "Validation Loss: 0.1196\n",
            "Validation Accuracy: 0.9735\n",
            "Epoch 6/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 250/250 [01:45<00:00,  2.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0039\n",
            "Validation Loss: 0.0675\n",
            "Validation Accuracy: 0.9825\n",
            "Epoch 7/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 250/250 [01:45<00:00,  2.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0009\n",
            "Validation Loss: 0.0636\n",
            "Validation Accuracy: 0.9835\n",
            "Epoch 8/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 250/250 [01:45<00:00,  2.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0004\n",
            "Validation Loss: 0.0614\n",
            "Validation Accuracy: 0.9845\n",
            "Epoch 9/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 250/250 [01:45<00:00,  2.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0004\n",
            "Validation Loss: 0.0597\n",
            "Validation Accuracy: 0.9835\n",
            "Epoch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 250/250 [01:45<00:00,  2.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0004\n",
            "Validation Loss: 0.0598\n",
            "Validation Accuracy: 0.9840\n"
          ]
        }
      ]
    }
  ]
}